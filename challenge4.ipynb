{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_version_hw4",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "USofkByQTCGl"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"ruoxinhuang\" \n",
        "os.environ['KAGGLE_KEY'] = \"f9d6d5aade71452503766f80812da73d\"\n",
        "!kaggle competitions download -c 11-785-fall-20-homework-4-part-2\n",
        "!unzip test.npy.zip\n",
        "!unzip train.npy.zip\n",
        "!unzip dev.npy.zip\n",
        "!unzip train_transcripts.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG9Dk-IrjXiH"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset \n",
        "import torch.nn as nn\n",
        "\n",
        "def load_data():\n",
        "    speech_train = np.load('train.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_valid = np.load('dev.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_test = np.load('test.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "    transcript_train = np.load('./train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "    transcript_valid = np.load('./dev_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "\n",
        "    return speech_train, speech_valid, speech_test, transcript_train, transcript_valid\n",
        "\n",
        "\n",
        "'''\n",
        "Transforms alphabetical input to numerical input, replace each letter by its corresponding \n",
        "index from letter_list\n",
        "'''\n",
        "def transform_letter_to_index(transcript, letter_list):\n",
        "    '''\n",
        "    :param transcript :(N, ) Transcripts are the text input\n",
        "    :param letter_list: Letter list defined above\n",
        "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
        "    '''\n",
        "    l2i, i2l = create_dictionaries(letter_list)\n",
        "\n",
        "    output = []\n",
        "    for sentence in transcript:\n",
        "        arr = []\n",
        "        for i, word in enumerate(sentence):\n",
        "            if i>0 and i<len(sentence):\n",
        "                arr.append(32)\n",
        "\n",
        "            for character in word.decode(\"utf-8\"):\n",
        "                arr.append(l2i[character])\n",
        "\n",
        "            if i==len(sentence)-1:\n",
        "                arr.append(34)\n",
        "        \n",
        "        output.append(arr)\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Optional, create dictionaries for letter2index and index2letter transformations\n",
        "'''\n",
        "def create_dictionaries(letter_list):\n",
        "    letter2index = dict()\n",
        "    index2letter = dict()\n",
        "    for idx, letter in enumerate(letter_list):\n",
        "        letter2index[letter] = idx\n",
        "        index2letter[idx] = letter\n",
        "\n",
        "    return letter2index, index2letter\n",
        "\n",
        "\n",
        "class Speech2TextDataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for the speech to text data, this may need some tweaking in the\n",
        "    getitem method as your implementation in the collate function may be different from\n",
        "    ours. \n",
        "    '''\n",
        "    def __init__(self, speech, text=None, isTrain=True):\n",
        "        self.speech = speech\n",
        "        self.isTrain = isTrain\n",
        "        if (text is not None):\n",
        "            self.text = text\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.speech.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if (self.isTrain == True):\n",
        "            return torch.tensor(self.speech[index].astype(np.float32)), len(self.speech[index]), torch.tensor(self.text[index]), len(self.text[index])\n",
        "        else:\n",
        "            return torch.tensor(self.speech[index].astype(np.float32)), len(self.speech[index])\n",
        "\n",
        "\n",
        "def collate_train(batch):\n",
        "    ### Return the padded speech and text data, and the length of utterance and transcript ###\n",
        "    X = [i[0] for i in batch]\n",
        "    X_len = [i[1] for i in batch]\n",
        "    Y = [i[2] for i in batch]\n",
        "    Y_len = [i[3] for i in batch]\n",
        "\n",
        "    X_pad = nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0.0)\n",
        "    Y_pad = nn.utils.rnn.pad_sequence(Y, batch_first=True, padding_value=0)\n",
        "\n",
        "\n",
        "    return X_pad,X_len,Y_pad,Y_len\n",
        "\n",
        "\n",
        "def collate_test(batch):\n",
        "    ### Return padded speech and length of utterance ###\n",
        "    X = [i[0] for i in batch]\n",
        "    X_len = [i[1] for i in batch]\n",
        "\n",
        "    X_pad = nn.utils.rnn.pad_sequence(X, batch_first=True, padding_value=0.0)\n",
        "\n",
        "\n",
        "    return X_pad,X_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii8eKT-d2Xf2"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "### Add Your Other Necessary Imports Here! ###\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    model.to(DEVICE) \n",
        "\n",
        "    total_loss = 0\n",
        "    for batch_num, (X, X_lens, Y, Y_lens) in enumerate(train_loader):\n",
        "        X = X.to(DEVICE)\n",
        "        Y = Y.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4) Pass your inputs, and length of speech into the model.\n",
        "        outputs = model(X, X_lens, Y, isTrain=True)\n",
        "        # 6) If necessary, reshape your predictions and origianl text input \n",
        "        outputs = outputs.permute(0, 2, 1)\n",
        "        # 7) Use the criterion to get the loss.\n",
        "        loss = criterion(outputs, Y)\n",
        "        # 9) Run the backward pass on the masked loss. \n",
        "        loss.backward()\n",
        "        # 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
        "        # 11) Take a step with your optimizer\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        if batch_num % 100 == 0:\n",
        "            print(total_loss/100)\n",
        "            total_loss = 0\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FIPuKmz2oKe"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key, value and query from Encoder and decoder.\n",
        "    Below are the set of operations you need to perform for computing attention:\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        self.softmax = nn.Softmax(1)\n",
        "\n",
        "    def forward(self, query, key, value, lens):\n",
        "        '''\n",
        "        :param query :(batch_size, hidden_size) Query is the output of LSTMCell from Decoder\n",
        "        :param keys: (batch_size, max_len, encoder_size) Key Projection from Encoder\n",
        "        :param values: (batch_size, max_len, encoder_size) Value Projection from Encoder\n",
        "        :return context: (batch_size, encoder_size) Attended Context\n",
        "        :return attention_mask: (batch_size, max_len) Attention mask that can be plotted  \n",
        "        '''\n",
        "        energy = torch.bmm(key, query.unsqueeze(2)).squeeze()\n",
        "        mask = torch.arange(key.shape[1]).unsqueeze(0) >= lens.unsqueeze(1)\n",
        "        mask = mask.to(DEVICE)\n",
        "        energy.masked_fill_(mask, -np.inf)\n",
        "        attention = self.softmax(energy)\n",
        "        context = torch.bmm(attention.unsqueeze(1), value).squeeze()\n",
        "        return context\n",
        "\n",
        "\n",
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
        "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
        "    and inferior results even after extensive training.\n",
        "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
        "    from a large number of input steps.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        '''\n",
        "        :param x :(N, T) input to the pBLSTM\n",
        "        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM \n",
        "        '''\n",
        "        batch_size, length, dim = x.shape\n",
        "        if(length%2==1):\n",
        "            x = x[:,:-1,:]\n",
        "\n",
        "        x = x.reshape(batch_size,int(length/2),dim*2)\n",
        "\n",
        "        out = utils.rnn.pack_padded_sequence(x, lengths=lens//2, batch_first=True, enforce_sorted=False)\n",
        "        out,_ = self.blstm(out)\n",
        "        out, out_lens = utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        \n",
        "        return out, out_lens\n",
        "        \n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key and value.\n",
        "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        ### Add code to define the blocks of pBLSTMs! ###\n",
        "        self.pblstm1 = pBLSTM(hidden_dim*4,hidden_dim)\n",
        "        self.pblstm2 = pBLSTM(hidden_dim*4,hidden_dim)\n",
        "        self.pblstm3 = pBLSTM(hidden_dim*4,hidden_dim)\n",
        "\n",
        "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
        "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=True, enforce_sorted=False)\n",
        "        out, _ = self.lstm(rnn_inp)\n",
        "        out, out_lens = utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "\n",
        "        ### Use the outputs and pass it through the pBLSTM blocks! ###\n",
        "\n",
        "        out, out_lens = self.pblstm1(out,out_lens)\n",
        "        out, out_lens = self.pblstm2(out,out_lens)\n",
        "        out, out_lens = self.pblstm3(out,out_lens)\n",
        "        \n",
        "        keys = self.key_network(out)\n",
        "        value = self.value_network(out)\n",
        "\n",
        "        return keys, value, out_lens\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
        "    thus we use LSTMCell instead of LSLTM here.\n",
        "    The output from the second LSTMCell can be used as query here for attention module.\n",
        "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
        "        self.tf = 0.05\n",
        "\n",
        "        self.isAttended = isAttended\n",
        "        if (isAttended == True):\n",
        "            self.attention = Attention()\n",
        "\n",
        "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
        "\n",
        "    def forward(self, key, values, lens, text=None, isTrain=True):\n",
        "        '''\n",
        "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
        "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
        "        :param text: (N, text_len) Batch input of text with text_length\n",
        "        :param isTrain: Train or eval mode\n",
        "        :return predictions: Returns the character perdiction probability \n",
        "        '''\n",
        "        batch_size = key.shape[0]\n",
        "\n",
        "        if (isTrain == True):\n",
        "            max_len =  text.shape[1]\n",
        "            embeddings = self.embedding(text)\n",
        "        else:\n",
        "            max_len = 600\n",
        "\n",
        "        predictions = []\n",
        "        hidden_states = [None, None]\n",
        "        prediction = torch.zeros((batch_size, self.vocab_size), device=DEVICE)\n",
        "        prediction[:, 33] = 1   \n",
        "        context = values[:, 0, :]\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # * Implement Gumble noise and teacher forcing techniques \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
        "            #   out of the loop so you do not get index out of range errors. \n",
        "            if i==0:\n",
        "              char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "            else:\n",
        "              if isTrain:\n",
        "                if random.random() > self.tf:\n",
        "                  char_embed = embeddings[:, i-1, :]\n",
        "                else:\n",
        "                  char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "              else:\n",
        "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "\n",
        "            inp = torch.cat([char_embed, context], dim=1)\n",
        "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
        "\n",
        "            inp_2 = hidden_states[0][0]\n",
        "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
        "\n",
        "            ### Compute attention from the output of the second LSTM Cell ###\n",
        "            output = hidden_states[1][0]\n",
        "            context = self.attention(output, key, values, lens)\n",
        "\n",
        "            with torch.no_grad():\n",
        "              self.character_prob.weight = self.embedding.weight\n",
        "            prediction = self.character_prob(torch.cat([output, context], dim=1))\n",
        "            predictions.append(prediction.unsqueeze(1))\n",
        "\n",
        "        return torch.cat(predictions, dim=1)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=True):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim, isAttended=True)\n",
        "\n",
        "    def forward(self, speech_input, speech_len, text_input=None, isTrain=True):\n",
        "        key, value, lens = self.encoder(speech_input, speech_len)\n",
        "        if (isTrain == True):\n",
        "            predictions = self.decoder(key, value, lens, text_input)\n",
        "        else:\n",
        "            predictions = self.decoder(key, value, lens, text=None, isTrain=False)\n",
        "        return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3xI33608GZV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "LETTER_LIST = ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
        "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLLJeaM35kiQ"
      },
      "source": [
        "speech_train, speech_valid, speech_test, transcript_train, transcript_valid = load_data()\n",
        "character_text_train = transform_letter_to_index(transcript_train, LETTER_LIST)\n",
        "character_text_valid = transform_letter_to_index(transcript_valid, LETTER_LIST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9hP-Xi2WnUI"
      },
      "source": [
        "model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=256, isAttended=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "nepochs = 60\n",
        "batch_size = 64 if DEVICE == 'cuda' else 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yguRfPweIEfm"
      },
      "source": [
        "train_dataset = Speech2TextDataset(speech_train, character_text_train)\n",
        "val_dataset = Speech2TextDataset(speech_valid, character_text_train)\n",
        "test_dataset = Speech2TextDataset(speech_test, None, False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_train)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WykcWt-nJAOF"
      },
      "source": [
        "for epoch in range(nepochs):\n",
        "    model.decoder.tf +=0.01  #this is commented out in the beginning\n",
        "    train(model, train_loader, criterion, optimizer, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEcxtj93xnnI"
      },
      "source": [
        "def test(model, test_loader):\n",
        "    ### Write your test code here! ###\n",
        "    model.eval()\n",
        "    result = []\n",
        "    for X, X_lens in test_loader:\n",
        "        X = X.to(DEVICE)\n",
        "\n",
        "        outputs = model(X, X_lens, isTrain=False)\n",
        "        indexes = torch.argmax(outputs, dim=-1).cpu()\n",
        "        \n",
        "        for i in indexes:\n",
        "          string = ''\n",
        "          for j in i:\n",
        "            if j == 34:\n",
        "              break\n",
        "            string += i2l[j.item()]\n",
        "        \n",
        "          result.append(string)\n",
        "      \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psDrCI-Oxf_9"
      },
      "source": [
        "l2i, i2l = create_dictionaries(LETTER_LIST)\n",
        "result = test(model, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVU4h_sqybHx"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('submission.csv', 'w') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "  csvwriter.writerow(('id','label'))\n",
        "  for i, row in enumerate(result):\n",
        "    csvwriter.writerow((i,row))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ2vyynMBK1e"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdDSLnHIpaIM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}